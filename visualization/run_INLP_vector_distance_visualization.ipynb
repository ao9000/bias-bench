{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f33d410fc545ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:06:26.941476Z",
     "start_time": "2024-08-06T19:06:26.353313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  7 03:06:26 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:01:00.0 Off |                  Off |\r\n",
      "| 30%   36C    P8              23W / 300W |      1MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:21:00.0 Off |                  Off |\r\n",
      "| 30%   34C    P8              19W / 300W |      1MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA RTX A6000               Off | 00000000:41:00.0 Off |                  Off |\r\n",
      "| 30%   37C    P8              16W / 300W |      1MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA RTX A6000               Off | 00000000:42:00.0 Off |                  Off |\r\n",
      "| 30%   33C    P8              15W / 300W |      1MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9733465777145860",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:06:26.946277Z",
     "start_time": "2024-08-06T19:06:26.943327Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974276b7dab558f",
   "metadata": {},
   "source": [
    "# Reference: https://github.com/shauli-ravfogel/nullspace_projection/blob/master/notebooks/word_vectors_debiasing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a7ebbdcaf2d167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:06:26.970676Z",
     "start_time": "2024-08-06T19:06:26.947198Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install scipy==1.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f1266e45c8b44",
   "metadata": {},
   "source": [
    "# Visualize the debiasing effect of INLP using stereotyped professions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4d4aef905fe1f",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe1fcb928cc572f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:06:26.982965Z",
     "start_time": "2024-08-06T19:06:26.972123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up variables (formerly command-line arguments)\n",
    "input_path = '/home/ongzhiyang/bias-bench-main/visualization/inlp_glove_embeddings/glove.42B.300d.txt'\n",
    "output_dir = \"/home/ongzhiyang/bias-bench-main/visualization/inlp_glove_embeddings/\"\n",
    "top_k = 150000\n",
    "keep_gendered = False\n",
    "keep_names = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52d7a884dc556fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:23:09.196382Z",
     "start_time": "2024-08-06T19:06:26.985069Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_764567/3355538505.py:20: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_filename, word2vec_output_file)\n",
      "1917494it [00:32, 59759.00it/s]\n",
      "150000it [00:18, 8000.38it/s]\n",
      "455it [00:00, 8167.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Files saved in the output directory.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import string\n",
    "import codecs\n",
    "import json\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def glove_to_word2vec(glove_filename, word2vec_output_file):\n",
    "    \"\"\"\n",
    "    Convert GloVe file to Word2Vec format and save the result.\n",
    "    :param glove_filename: Path to the GloVe file.\n",
    "    :param word2vec_output_file: Path to the output Word2Vec format file.\n",
    "    \"\"\"\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove2word2vec(glove_filename, word2vec_output_file)\n",
    "\n",
    "def get_excluded_words():\n",
    "    \"\"\"\n",
    "    :return: excluded words, a list of words we possibly want to remove.\n",
    "             the list of excluded words is based on Gonen et at. 2019 and contains gender specific words.\n",
    "    \"\"\"\n",
    "    with codecs.open('/home/ongzhiyang/nullspace_projection-master/data/lists/gender_specific_full.json') as f:\n",
    "        gender_specific = json.load(f)\n",
    "\n",
    "    exclude_words = list(set(gender_specific))\n",
    "    return exclude_words\n",
    "\n",
    "def get_names():\n",
    "    \"\"\"\n",
    "    A list of gender-specific first names.\n",
    "    \"\"\"\n",
    "    with open(\"/home/ongzhiyang/nullspace_projection-master/data/lists/first_names_clean.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    names = [l.strip().lower() for l in lines]\n",
    "    return names\n",
    "\n",
    "def load_model(fname, binary) -> Tuple[gensim.models.keyedvectors.KeyedVectors, List[np.ndarray], List[str]]:\n",
    "    \"\"\"\n",
    "    :param fname: str, the filename of the embeddings file\n",
    "    :param binary: whether the format is binary\n",
    "    :return: a tuple model:gensim.models.keyedvectors.WordEmbeddingsKeyedVectors, vecs: list of np arrays,\n",
    "            words: list of strings\n",
    "    \"\"\"\n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=binary)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.key_to_index.keys())\n",
    "\n",
    "    return model, vecs, words\n",
    "\n",
    "def has_punct(w: str):\n",
    "    \"\"\"\n",
    "    :param w: a word\n",
    "    :return: True if contains punctuation, False otherwise\n",
    "    \"\"\"\n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    \"\"\"\n",
    "    :param w: a word\n",
    "    :return: True if contains digit, False otherwise\n",
    "    \"\"\"\n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def save_in_word2vec_format(vecs: np.ndarray, words: np.ndarray, fname: str):\n",
    "    \"\"\"\n",
    "    :param vecs: np array of vectors\n",
    "    :param words: np array of strings\n",
    "    :param fname: fname to save\n",
    "    :return:\n",
    "    saves the vectors in a word2vec format\n",
    "    \"\"\"\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(str(len(vecs)) + \" \" + \"300\" + \"\\n\")\n",
    "        for i, (v, w) in tqdm.tqdm(enumerate(zip(vecs, words))):\n",
    "            vec_as_str = \" \".join([str(x) for x in v])\n",
    "            f.write(w + \" \" + vec_as_str + \"\\n\")\n",
    "\n",
    "def filter_vecs(vecs: np.ndarray, words: np.ndarray, keep_gendered: bool, keep_names: bool):\n",
    "    \"\"\"\n",
    "    :param vecs: the complete set of pretrained vectors\n",
    "    :param words: the corresponding words\n",
    "    :param keep_gendered: whether or not to keep inherently gendered words\n",
    "    :param keep_names:  whether or not to keep names\n",
    "    :return: two tuples, one contains the filtered vecs and words, the other only the gendered ones\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    gendered_words = []\n",
    "\n",
    "    excluded_words = get_excluded_words()\n",
    "    first_names = set(get_names())\n",
    "\n",
    "    for i, (v, w) in tqdm.tqdm(enumerate(zip(vecs, words))):\n",
    "        if (w in first_names) and not keep_names:\n",
    "            continue\n",
    "\n",
    "        if w in excluded_words:\n",
    "            gendered_words.append((w, v))\n",
    "            if not keep_gendered:\n",
    "                continue\n",
    "\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                filtered.append((w, v))\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "\n",
    "        filtered.append((w, v))\n",
    "\n",
    "    words, vecs = zip(*filtered)\n",
    "    vecs = np.array(vecs)\n",
    "    words = list(words)\n",
    "\n",
    "    words_gendered, vecs_gendered = zip(*gendered_words)\n",
    "    vecs_gendered = np.array(vecs_gendered)\n",
    "    words_gendered = list(words_gendered)\n",
    "\n",
    "    return (vecs, words), (words_gendered, vecs_gendered)\n",
    "\n",
    "def save_voc(voc: List[str], path: str):\n",
    "    \"\"\"\n",
    "    :param voc: list of words in the vocab\n",
    "    :param path: path to save\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for w in voc:\n",
    "            f.write(w + \"\\n\")\n",
    "\n",
    "# Main execution\n",
    "\n",
    "# Convert GloVe to Word2Vec format\n",
    "word2vec_file = output_dir + \"word2vec.txt\"\n",
    "glove_to_word2vec(input_path, word2vec_file)\n",
    "\n",
    "# Load the model\n",
    "model, vecs, words = load_model(word2vec_file, binary=False)\n",
    "\n",
    "# Filter vectors\n",
    "(vecs, words), (words_gendered, vecs_gendered) = filter_vecs(vecs, words, keep_gendered, keep_names)\n",
    "\n",
    "# Save filtered vectors\n",
    "save_in_word2vec_format(vecs[:top_k], words, output_dir + \"vecs.filtered.txt\")\n",
    "\n",
    "# Save gendered vectors\n",
    "save_in_word2vec_format(vecs_gendered, words_gendered, output_dir + \"vecs.gendered.txt\")\n",
    "\n",
    "# Save vocabulary\n",
    "save_voc(words[:top_k], output_dir + \"voc.txt\")\n",
    "\n",
    "print(\"Processing complete. Files saved in the output directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f57d069cf135515",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f997fb017c0f966",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceaeebe5808a8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:23:09.751967Z",
     "start_time": "2024-08-06T19:23:09.197956Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_764567/1439854310.py:12: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"inlp_glove_embeddings\")\n",
    "import debias\n",
    "import codecs\n",
    "import json\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats.stats import pearsonr\n",
    "import tqdm\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['agg.path.chunksize'] = 10000\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421904063a8a8a52",
   "metadata": {},
   "source": [
    "### Data loading & processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58295d6c885aa06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:23:09.763170Z",
     "start_time": "2024-08-06T19:23:09.753501Z"
    }
   },
   "outputs": [],
   "source": [
    "# def load_word_vectors(fname):\n",
    "#\n",
    "#     model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "#     vecs = model.vectors\n",
    "#     words = list(model.vocab.keys())\n",
    "#     return model, vecs, words\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def load_word_vectors(fname):\n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    vecs = model.vectors\n",
    "    words = model.index_to_key\n",
    "    return model, vecs, words\n",
    "\n",
    "def project_on_gender_subspaces(gender_vector, model: Word2VecKeyedVectors, n = 2500):\n",
    "    \n",
    "    group1 = model.similar_by_vector(gender_vector, topn = n, restrict_vocab=None)\n",
    "    group2 = model.similar_by_vector(-gender_vector, topn = n, restrict_vocab=None)\n",
    "    \n",
    "    all_sims = model.similar_by_vector(gender_vector, topn = len(model.vectors), restrict_vocab=None)\n",
    "    eps = 0.03\n",
    "    idx = [i for i in range(len(all_sims)) if abs(all_sims[i][1]) < eps]\n",
    "    samp = set(np.random.choice(idx, size = n))\n",
    "    neut = [s for i,s in enumerate(all_sims) if i in samp]\n",
    "    return group1, group2, neut\n",
    "\n",
    "def get_vectors(word_list: list, model: Word2VecKeyedVectors):\n",
    "    \n",
    "    vecs = []\n",
    "    for w in word_list:\n",
    "        \n",
    "        vecs.append(model[w])\n",
    "    \n",
    "    vecs = np.array(vecs)\n",
    "\n",
    "    return vecs\n",
    "    \n",
    "def get_bias_by_neighbors(model, v, gender_direction, topn): \n",
    "    \n",
    "    neighbors = model.similar_by_vector(v, topn = topn) \n",
    "    neighbors_words = [n for n, _ in neighbors]\n",
    "    \n",
    "    #bias = len([n for n in neighbors_words if n in gendered_words])\n",
    "    bias = len([n for n in neighbors_words if model.cosine_similarities(model[n], [gender_direction])[0] > 0])\n",
    "    bias /= (1.*topn)\n",
    "    return bias\n",
    "\n",
    "\n",
    "def save_in_word2vec_format(vecs: np.ndarray, words: np.ndarray, fname: str):\n",
    "\n",
    "\n",
    "    with open(fname, \"w\", encoding = \"utf-8\") as f:\n",
    "\n",
    "        f.write(str(len(vecs)) + \" \" + \"300\" + \"\\n\")\n",
    "        for i, (v,w) in tqdm.tqdm_notebook(enumerate(zip(vecs, words))):\n",
    "\n",
    "            vec_as_str = \" \".join([str(x) for x in v])\n",
    "            f.write(w + \" \" + vec_as_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881afa84514af4a6",
   "metadata": {},
   "source": [
    "### Load word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba45d5c4a2be368a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:23:37.113941Z",
     "start_time": "2024-08-06T19:23:09.764460Z"
    }
   },
   "outputs": [],
   "source": [
    "# 150k top vectors (with gender-typical words) - used for training\n",
    "\n",
    "model, vecs, words = load_word_vectors(fname = \"inlp_glove_embeddings/vecs.filtered.txt\")\n",
    "\n",
    "# only gendered vectors\n",
    "\n",
    "model_gendered, _, _ = load_word_vectors(fname = \"inlp_glove_embeddings/vecs.gendered.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8d9de71361970",
   "metadata": {},
   "source": [
    "### Collect biased words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a7ec332ba63723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T19:23:37.561589Z",
     "start_time": "2024-08-06T19:23:37.115498Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'he' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     gender_direction \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mcomponents_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     gender_direction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39mmodel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshe\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[1;32m     15\u001b[0m gender_unit_vec \u001b[38;5;241m=\u001b[39m gender_direction\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(gender_direction)\n\u001b[1;32m     16\u001b[0m masc_words_and_scores, fem_words_and_scores, neut_words_and_scores \u001b[38;5;241m=\u001b[39m project_on_gender_subspaces(gender_direction, model, n \u001b[38;5;241m=\u001b[39m num_vectors_per_class)\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'he' not present\""
     ]
    }
   ],
   "source": [
    "num_vectors_per_class = 7500\n",
    "\n",
    "by_pca = False\n",
    "if by_pca:\n",
    "    pairs = [(\"male\", \"female\"), (\"masculine\", \"feminine\"), (\"he\", \"she\"), (\"him\", \"her\")]\n",
    "    gender_vecs = [model[p[0]] - model[p[1]] for p in pairs]\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(gender_vecs)\n",
    "    gender_direction = pca.components_[0]\n",
    "\n",
    "else:\n",
    "    gender_direction = model[\"he\"]-model[\"she\"] \n",
    "\n",
    "\n",
    "gender_unit_vec = gender_direction/np.linalg.norm(gender_direction)\n",
    "masc_words_and_scores, fem_words_and_scores, neut_words_and_scores = project_on_gender_subspaces(gender_direction, model, n = num_vectors_per_class)\n",
    "\n",
    "masc_words, masc_scores = list(zip(*masc_words_and_scores))\n",
    "neut_words, neut_scores = list(zip(*neut_words_and_scores))\n",
    "fem_words, fem_scores = list(zip(*fem_words_and_scores))\n",
    "masc_vecs, fem_vecs = get_vectors(masc_words, model), get_vectors(fem_words, model)\n",
    "neut_vecs = get_vectors(neut_words, model)\n",
    "\n",
    "# n = min(3000, num_vectors_per_class)\n",
    "# all_significantly_biased_words = masc_words[:n] + fem_words[:n]\n",
    "# all_significantly_biased_vecs =  np.concatenate((masc_vecs[:n], fem_vecs[:n]))\n",
    "# all_significantly_biased_labels = np.concatenate((np.ones(n, dtype = int),\n",
    "#                                                   np.zeros(n, dtype = int)))\n",
    "# \n",
    "# all_significantly_biased_words, all_significantly_biased_vecs, all_significantly_biased_labels = sklearn.utils.shuffle(\n",
    "# all_significantly_biased_words, all_significantly_biased_vecs, all_significantly_biased_labels)\n",
    "# #print(np.random.choice(masc_words, size = 75))\n",
    "# print(\"TOP MASC\")\n",
    "# print(masc_words[:50])\n",
    "# #print(\"LAST MASC\")\n",
    "# #print(masc_words[-120:])\n",
    "# print(\"-------------------------\")\n",
    "# #print(np.random.choice(fem_words, size = 75))\n",
    "# print(\"TOP FEM\")\n",
    "# print(fem_words[:50])\n",
    "# #print(\"LAST FEM\")\n",
    "# #print(fem_words[-120:])\n",
    "# print(\"-------------------------\")\n",
    "# #print(np.random.choice(neut_words, size = 75))\n",
    "# print(neut_words[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b97d80741122f",
   "metadata": {},
   "source": [
    "### Perform train-dev-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c32356a6d4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.concatenate((masc_vecs, fem_vecs, neut_vecs), axis = 0)\n",
    "#X = (X - np.mean(X, axis = 0, keepdims = True)) / np.std(X, axis = 0)\n",
    "y_masc = np.ones(masc_vecs.shape[0], dtype = int)\n",
    "y_fem = np.zeros(fem_vecs.shape[0], dtype = int)\n",
    "y_neut = -np.ones(neut_vecs.shape[0], dtype = int)\n",
    "#y = np.concatenate((masc_scores, fem_scores, neut_scores))#np.concatenate((y_masc, y_fem))\n",
    "y = np.concatenate((y_masc, y_fem, y_neut))\n",
    "X_train_dev, X_test, y_train_dev, Y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "X_train, X_dev, Y_train, Y_dev = sklearn.model_selection.train_test_split(X_train_dev, y_train_dev, test_size = 0.3, random_state = 0)\n",
    "print(\"Train size: {}; Dev size: {}; Test size: {}\".format(X_train.shape[0], X_dev.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf108ca868f344",
   "metadata": {},
   "source": [
    "### Debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2619432f3f7d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_clf = LinearSVC\n",
    "#gender_clf = SGDClassifier\n",
    "#gender_clf = LogisticRegression\n",
    "#gender_clf = LinearDiscriminantAnalysis\n",
    "#gender_clf = Perceptron\n",
    "\n",
    "params_svc = {'fit_intercept': False, 'class_weight': None, \"dual\": False, 'random_state': 0}\n",
    "params_sgd = {'fit_intercept': False, 'class_weight': None, 'max_iter': 1000, 'random_state': 0}\n",
    "params = params_svc\n",
    "#params = {'loss': 'hinge', 'n_jobs': 16, 'penalty': 'l2', 'max_iter': 2500, 'random_state': 0}\n",
    "#params = {}\n",
    "n = 35\n",
    "min_acc = 0\n",
    "is_autoregressive = True\n",
    "dropout_rate = 0\n",
    "\n",
    "P, rowspace_projs, Ws = debias.get_debiasing_projection(gender_clf, params, n, 300, is_autoregressive, min_acc,\n",
    "                                    X_train, Y_train, X_dev, Y_dev,\n",
    "                                       Y_train_main=None, Y_dev_main=None, \n",
    "                                        by_class = False, dropout_rate = dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214de13b71d8c24",
   "metadata": {},
   "source": [
    "### Save debiased vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139141da67c2d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_cleaned = (P.dot(vecs.T)).T\n",
    "save_in_word2vec_format(vecs_cleaned, words, \"inlp_glove_embeddings/vecs.150k.cleaned.txt\")\n",
    "model_cleaned, _, _ = load_word_vectors(fname = \"inlp_glove_embeddings/vecs.150k.cleaned.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb20b2c1fcc58c1",
   "metadata": {},
   "source": [
    "#### Plot distance of biased profession word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f25baf67555a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def plot_profession_pronoun_distances(model, model_cleaned, P, professions, male_dominated, female_dominated, output_dir='.'):\n",
    "    # Calculate distances\n",
    "    him_vec, her_vec = model['he'], model['she']\n",
    "    him_vec_cleaned, her_vec_cleaned = model_cleaned['he'], model_cleaned['she']\n",
    "\n",
    "    distances_him_before, distances_her_before = [], []\n",
    "    distances_him_after, distances_her_after = [], []\n",
    "    valid_professions = []\n",
    "\n",
    "    for profession in professions:\n",
    "        if profession not in model:\n",
    "            print(f\"Warning: '{profession}' not found in the model. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        prof_vec = model[profession]\n",
    "        prof_vec_cleaned = np.dot(P, prof_vec)\n",
    "\n",
    "        distances_him_before.append(np.linalg.norm(prof_vec - him_vec))\n",
    "        distances_her_before.append(np.linalg.norm(prof_vec - her_vec))\n",
    "        distances_him_after.append(np.linalg.norm(prof_vec_cleaned - him_vec_cleaned))\n",
    "        distances_her_after.append(np.linalg.norm(prof_vec_cleaned - her_vec_cleaned))\n",
    "        valid_professions.append(profession)\n",
    "\n",
    "    # Create two separate plots\n",
    "    for distances_him, distances_her, title in [\n",
    "        (distances_him_before, distances_her_before, \"Before Debiasing\"),\n",
    "        (distances_him_after, distances_her_after, \"After Debiasing\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        for i, profession in enumerate(valid_professions):\n",
    "            color = 'blue' if profession in male_dominated else 'red' if profession in female_dominated else 'gray'\n",
    "            ax.scatter(distances_him[i], distances_her[i], alpha=0.7, color=color)\n",
    "            ax.annotate(profession, (distances_him[i], distances_her[i]), fontsize=8)\n",
    "\n",
    "        ax.set_xlabel(\"Distance to 'him' vector\")\n",
    "        ax.set_ylabel(\"Distance to 'her' vector\")\n",
    "        ax.set_title(f\"Profession-Pronoun Distances: {title}\")\n",
    "        \n",
    "        # Plot diagonal line\n",
    "        min_val = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "        max_val = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=1)\n",
    "        \n",
    "        # Set equal aspect ratio\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        # Add correlation to plot\n",
    "        corr, _ = pearsonr(distances_him, distances_her)\n",
    "        ax.text(0.05, 0.95, f'Correlation: {corr:.4f}', transform=ax.transAxes, \n",
    "                verticalalignment='top', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "        # Add legend\n",
    "        custom_lines = [plt.Line2D([0], [0], color='blue', lw=4),\n",
    "                        plt.Line2D([0], [0], color='red', lw=4),\n",
    "                        plt.Line2D([0], [0], color='gray', lw=4)]\n",
    "        ax.legend(custom_lines, ['Male-dominated', 'Female-dominated', 'Neutral'], \n",
    "                  loc='lower right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        filename = f'profession_pronoun_distances_{title.lower().replace(\" \", \"_\")}.png'\n",
    "        filepath = f'{output_dir}/{filename}'\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        # Show\n",
    "        plt.show()\n",
    "        print(f\"Saved chart: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea52df62b9b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_dominated = {'mechanic', 'software', 'farmer', 'construction', 'finance', 'aerospace', 'architect', 'pilot', 'engineer', 'firefighter', 'carpenter'}\n",
    "female_dominated = {'preschool', 'kindergarten', 'childcare', 'skincare', 'receptionist', 'hairdresser', 'nurse', 'teacher', 'maid', 'cleaner'}\n",
    "custom_professions = list(male_dominated | female_dominated)\n",
    "\n",
    "plot_profession_pronoun_distances(model, model_cleaned, P, custom_professions, male_dominated, female_dominated, output_dir='media/inlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c192ff3c16014",
   "metadata": {},
   "source": [
    "# Summary of code\n",
    "\n",
    "## 1. Data Preparation\n",
    "- Converts GloVe embeddings to Word2Vec format for easier processing.\n",
    "- Filters the vocabulary, optionally keeping or removing gendered words and names.\n",
    "- Saves the filtered embeddings and vocabulary.\n",
    "\n",
    "## 2. Loading and Processing\n",
    "- Loads the filtered word vectors and gendered vectors.\n",
    "- Identifies gender-biased words by projecting words onto a gender direction (defined by the difference between \"he\" and \"she\" embeddings).\n",
    "- Separates words into masculine, feminine, and neutral categories based on their projection onto the gender direction.\n",
    "\n",
    "## 3. Train-Dev-Test Split\n",
    "- Splits the data into training, development, and test sets for the debiasing process.\n",
    "\n",
    "## 4. Debiasing\n",
    "- Uses the Iterative Nullspace Projection (INLP) method to debias the word embeddings.\n",
    "- Trains a series of linear classifiers (SVC) to identify and remove gender direction from the embeddings.\n",
    "\n",
    "## 5. Saving Debiased Vectors\n",
    "- Applies the learned projection to the original vectors to create debiased embeddings.\n",
    "- Saves the debiased embeddings in Word2Vec format.\n",
    "\n",
    "## 6. Visualization\n",
    "- Includes a function to plot the distances of profession word vectors to gendered pronouns (\"he\" and \"she\") before and after debiasing.\n",
    "- Calculates and displays the correlation between these distances to show the effect of debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b464591462a738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fyp] *",
   "language": "python",
   "name": "conda-env-fyp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
