{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf28afe232aec985",
   "metadata": {},
   "source": [
    "# Run self-debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T08:35:11.255439Z",
     "start_time": "2024-06-06T08:33:45.764076Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/FYP/on0008an/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Parameters: Namespace(prompts_filename='/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts_tiny.jsonl', output_dir='/home/FYP/on0008an/bias-bench-main/results/self-debias', models=['gpt2'], modes=['default', 'debiased'], decay_constant=50, epsilon=0.01, do_sample=False, min_length=20, max_length=20, top_k=5, num_beams=3, num_return_sequences=1, num_repeats=1, not_challenging_only=True, use_keywords=False, max_prompts=-1, seed=1, debug=False)\r\n",
      "Loading prompts from file \"/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts_tiny.jsonl\"\r\n",
      "Done loading 39 prompts from file \"/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts_tiny.jsonl\"\r\n",
      "Generating continuations for 39 prompts with model gpt2 (default)\r\n",
      "Prompts:   0%|                                           | 0/39 [00:00<?, ?it/s]/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\r\n",
      "  warnings.warn(\r\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:   3%|▉                                  | 1/39 [00:01<00:43,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:   5%|█▊                                 | 2/39 [00:01<00:28,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:   8%|██▋                                | 3/39 [00:02<00:22,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  10%|███▌                               | 4/39 [00:02<00:19,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  13%|████▍                              | 5/39 [00:03<00:17,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  15%|█████▍                             | 6/39 [00:03<00:16,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  18%|██████▎                            | 7/39 [00:03<00:15,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  21%|███████▏                           | 8/39 [00:04<00:14,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  23%|████████                           | 9/39 [00:04<00:14,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  26%|████████▋                         | 10/39 [00:05<00:13,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  28%|█████████▌                        | 11/39 [00:05<00:13,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  31%|██████████▍                       | 12/39 [00:06<00:12,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  33%|███████████▎                      | 13/39 [00:06<00:12,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  36%|████████████▏                     | 14/39 [00:07<00:11,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  38%|█████████████                     | 15/39 [00:07<00:11,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  41%|█████████████▉                    | 16/39 [00:08<00:10,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  44%|██████████████▊                   | 17/39 [00:08<00:10,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  46%|███████████████▋                  | 18/39 [00:09<00:09,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  49%|████████████████▌                 | 19/39 [00:09<00:09,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  51%|█████████████████▍                | 20/39 [00:10<00:09,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  54%|██████████████████▎               | 21/39 [00:10<00:08,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  56%|███████████████████▏              | 22/39 [00:10<00:07,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  59%|████████████████████              | 23/39 [00:11<00:07,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  62%|████████████████████▉             | 24/39 [00:11<00:06,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  64%|█████████████████████▊            | 25/39 [00:12<00:06,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  67%|██████████████████████▋           | 26/39 [00:12<00:06,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  69%|███████████████████████▌          | 27/39 [00:13<00:05,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  72%|████████████████████████▍         | 28/39 [00:13<00:05,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  74%|█████████████████████████▎        | 29/39 [00:14<00:04,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  77%|██████████████████████████▏       | 30/39 [00:14<00:04,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  79%|███████████████████████████       | 31/39 [00:15<00:03,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  82%|███████████████████████████▉      | 32/39 [00:15<00:03,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  85%|████████████████████████████▊     | 33/39 [00:16<00:02,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  87%|█████████████████████████████▋    | 34/39 [00:16<00:02,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  90%|██████████████████████████████▌   | 35/39 [00:17<00:01,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  92%|███████████████████████████████▍  | 36/39 [00:17<00:01,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  95%|████████████████████████████████▎ | 37/39 [00:17<00:00,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  97%|█████████████████████████████████▏| 38/39 [00:18<00:00,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts: 100%|██████████████████████████████████| 39/39 [00:18<00:00,  2.06it/s]\r\n",
      "=== RESULT [gpt2, default] ===\r\n",
      "{'toxicity': 0.22078210207692303, 'severe_toxicity': 0.06594697424102565, 'sexually_explicit': 0.09262154524102566, 'threat': 0.10008070068717949, 'profanity': 0.12412709982051282, 'identity_attack': 0.08196558466666667}\r\n",
      "{'toxicity_probability': 0.15384615384615385, 'severe_toxicity_probability': 0.05128205128205128, 'sexually_explicit_probability': 0.07692307692307693, 'threat_probability': 0.07692307692307693, 'profanity_probability': 0.05128205128205128, 'identity_attack_probability': 0.05128205128205128}\r\n",
      "Generating continuations for 39 prompts with model gpt2 (debiased)\r\n",
      "Prompts:   0%|                                           | 0/39 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:   3%|▉                                  | 1/39 [00:00<00:23,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:   5%|█▊                                 | 2/39 [00:01<00:21,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:   8%|██▋                                | 3/39 [00:01<00:20,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  10%|███▌                               | 4/39 [00:02<00:20,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  13%|████▍                              | 5/39 [00:02<00:19,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  15%|█████▍                             | 6/39 [00:03<00:18,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  18%|██████▎                            | 7/39 [00:04<00:18,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  21%|███████▏                           | 8/39 [00:04<00:17,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'quota_location': 'global', 'service': 'commentanalyzer.googleapis.com', 'quota_metric': 'CommentAnalyzerService/analyze_requests', 'quota_limit_value': '60', 'quota_limit': 'AnalyzeRequestsPerMinutePerProject', 'consumer': 'projects/352421482365'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'quota_metric': 'CommentAnalyzerService/analyze_requests', 'quota_location': 'global', 'quota_limit_value': '60', 'service': 'commentanalyzer.googleapis.com', 'consumer': 'projects/352421482365', 'quota_limit': 'AnalyzeRequestsPerMinutePerProject'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Prompts:  23%|████████                           | 9/39 [00:15<01:50,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  26%|████████▋                         | 10/39 [00:15<01:19,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  28%|█████████▌                        | 11/39 [00:16<00:57,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'quota_metric': 'CommentAnalyzerService/analyze_requests', 'quota_limit_value': '60', 'service': 'commentanalyzer.googleapis.com', 'quota_limit': 'AnalyzeRequestsPerMinutePerProject', 'quota_location': 'global', 'consumer': 'projects/352421482365'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Prompts:  31%|██████████▍                       | 12/39 [00:21<01:24,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'quota_limit': 'AnalyzeRequestsPerMinutePerProject', 'quota_location': 'global', 'quota_limit_value': '60', 'consumer': 'projects/352421482365', 'quota_metric': 'CommentAnalyzerService/analyze_requests', 'service': 'commentanalyzer.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Prompts:  33%|███████████▎                      | 13/39 [00:27<01:40,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'consumer': 'projects/352421482365', 'quota_location': 'global', 'quota_limit': 'AnalyzeRequestsPerMinutePerProject', 'quota_limit_value': '60', 'service': 'commentanalyzer.googleapis.com', 'quota_metric': 'CommentAnalyzerService/analyze_requests'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Prompts:  36%|████████████▏                     | 14/39 [00:32<01:49,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  38%|█████████████                     | 15/39 [00:33<01:17,  3.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  41%|█████████████▉                    | 16/39 [00:34<00:55,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  44%|██████████████▊                   | 17/39 [00:34<00:40,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  46%|███████████████▋                  | 18/39 [00:35<00:30,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  49%|████████████████▌                 | 19/39 [00:35<00:23,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  51%|█████████████████▍                | 20/39 [00:36<00:19,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'consumer': 'projects/352421482365', 'quota_limit_value': '60', 'quota_limit': 'AnalyzeRequestsPerMinutePerProject', 'quota_location': 'global', 'service': 'commentanalyzer.googleapis.com', 'quota_metric': 'CommentAnalyzerService/analyze_requests'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Prompts:  54%|██████████████████▎               | 21/39 [00:41<00:42,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  56%|███████████████████▏              | 22/39 [00:42<00:31,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  59%|████████████████████              | 23/39 [00:42<00:23,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  62%|████████████████████▉             | 24/39 [00:43<00:17,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Perspective API threw an error: <HttpError 429 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw&alt=json returned \"Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:352421482365'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'quota_metric': 'CommentAnalyzerService/analyze_requests', 'quota_limit_value': '60', 'service': 'commentanalyzer.googleapis.com', 'quota_limit': 'AnalyzeRequestsPerMinutePerProject', 'quota_location': 'global', 'consumer': 'projects/352421482365'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]\">\r\n",
      " Retrying in 5 seconds...\r\n",
      "Prompts:  64%|█████████████████████▊            | 25/39 [00:49<00:34,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  67%|██████████████████████▋           | 26/39 [00:49<00:24,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  69%|███████████████████████▌          | 27/39 [00:50<00:18,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  72%|████████████████████████▍         | 28/39 [00:50<00:13,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  74%|█████████████████████████▎        | 29/39 [00:51<00:10,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  77%|██████████████████████████▏       | 30/39 [00:52<00:08,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  79%|███████████████████████████       | 31/39 [00:52<00:06,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  82%|███████████████████████████▉      | 32/39 [00:53<00:05,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  85%|████████████████████████████▊     | 33/39 [00:53<00:04,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  87%|█████████████████████████████▋    | 34/39 [00:54<00:03,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  90%|██████████████████████████████▌   | 35/39 [00:54<00:02,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  92%|███████████████████████████████▍  | 36/39 [00:55<00:01,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  95%|████████████████████████████████▎ | 37/39 [00:55<00:01,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts:  97%|█████████████████████████████████▏| 38/39 [00:56<00:00,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n",
      "Prompts: 100%|██████████████████████████████████| 39/39 [00:57<00:00,  1.46s/it]\r\n",
      "=== RESULT [gpt2, debiased] ===\r\n",
      "{'toxicity': 0.09898580973076924, 'severe_toxicity': 0.029932385516410254, 'sexually_explicit': 0.017832650643589736, 'threat': 0.05362478749487181, 'profanity': 0.042055580256410256, 'identity_attack': 0.052673861530769236}\r\n",
      "{'toxicity_probability': 0.05128205128205128, 'severe_toxicity_probability': 0.02564102564102564, 'sexually_explicit_probability': 0.0, 'threat_probability': 0.05128205128205128, 'profanity_probability': 0.0, 'identity_attack_probability': 0.05128205128205128}\r\n"
     ]
    }
   ],
   "source": [
    "!python self_debiasing.py --prompts_filename \"/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts.jsonl\" --output_dir \"/home/FYP/on0008an/bias-bench-main/results/self-debias\" --models \"gpt2\" --seed 1 --not_challenging_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5bb6506fc2497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T06:04:33.724358Z",
     "start_time": "2024-06-06T06:04:13.249644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: Namespace(prompts_filename='/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts.jsonl', output_dir='/home/FYP/on0008an/bias-bench-main/results/self-debias', api_key='AIzaSyBHepWTrfX0oEBT1ugIs4XMiOIzSKfZqTw', models=['meta-llama/Llama-2-7b-chat-hf'], modes=['default', 'debiased'], decay_constant=50, epsilon=0.01, do_sample=False, min_length=20, max_length=20, top_k=5, num_beams=3, num_return_sequences=1, num_repeats=1, not_challenging_only=False, use_keywords=False, max_prompts=-1, seed=1, debug=False)\r\n",
      "Loading prompts from file \"/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts.jsonl\"\r\n",
      "Done loading 1199 challenging prompts from file \"/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts.jsonl\"\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.11s/it]\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/FYP/on0008an/bias-bench-main/bias_bench/debias/self_debias/self_debiasing.py\", line 205, in <module>\r\n",
      "    wrapper = Llama2Wrapper(model_name=model_name)\r\n",
      "  File \"/home/FYP/on0008an/bias-bench-main/bias_bench/debias/self_debias/modeling.py\", line 761, in __init__\r\n",
      "    self._model.to(self._device)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2724, in to\r\n",
      "    return super().to(*args, **kwargs)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1173, in to\r\n",
      "    return self._apply(convert)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 779, in _apply\r\n",
      "    module._apply(fn)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 779, in _apply\r\n",
      "    module._apply(fn)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 779, in _apply\r\n",
      "    module._apply(fn)\r\n",
      "  [Previous line repeated 2 more times]\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 804, in _apply\r\n",
      "    param_applied = fn(param)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1159, in convert\r\n",
      "    return t.to(\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python self_debiasing.py --prompts_filename \"/home/FYP/on0008an/Bias_LLM_scripts/Bias Evaluation/Datasets/RealToxicityPrompts-Dataset/prompts.jsonl\" --output_dir \"/home/FYP/on0008an/bias-bench-main/results/self-debias\" --models \"meta-llama/Llama-2-7b-chat-hf\" --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a4930e4a2503b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
