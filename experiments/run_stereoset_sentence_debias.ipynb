{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470c846704abd930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:15:53.759108Z",
     "start_time": "2024-08-03T08:15:53.050046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  3 16:15:53 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:01:00.0 Off |                  Off |\r\n",
      "| 54%   74C    P2             233W / 300W |  15978MiB / 49140MiB |     47%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:21:00.0 Off |                  Off |\r\n",
      "| 30%   38C    P8              20W / 300W |  14880MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA RTX A6000               Off | 00000000:41:00.0 Off |                  Off |\r\n",
      "| 30%   44C    P2              81W / 300W |  42063MiB / 49140MiB |     21%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA RTX A6000               Off | 00000000:42:00.0 Off |                  Off |\r\n",
      "| 31%   62C    P2             181W / 300W |   4594MiB / 49140MiB |     94%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A   2852658      C   python                                     1264MiB |\r\n",
      "|    0   N/A  N/A   2914029      C   ...e/vda/anaconda3/envs/mwp/bin/python      360MiB |\r\n",
      "|    0   N/A  N/A   2915185      C   ...e/vda/anaconda3/envs/mwp/bin/python    14342MiB |\r\n",
      "|    1   N/A  N/A   2914029      C   ...e/vda/anaconda3/envs/mwp/bin/python    14874MiB |\r\n",
      "|    2   N/A  N/A   2852658      C   python                                    41418MiB |\r\n",
      "|    2   N/A  N/A   3170415      C   python                                      636MiB |\r\n",
      "|    3   N/A  N/A   2089965      C   python                                     3084MiB |\r\n",
      "|    3   N/A  N/A   3161806      C   python                                      614MiB |\r\n",
      "|    3   N/A  N/A   3165973      C   python                                      884MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80dd17b55805a50d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:16:00.899353Z",
     "start_time": "2024-08-03T08:16:00.894727Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a46c963b5e61c",
   "metadata": {},
   "source": [
    "# Run Sentence Debiasing on StereoSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de40074a7cbf635d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:25:39.582992Z",
     "start_time": "2024-08-03T08:16:02.851951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python stereoset_debias.py --bias_type 'gender' --model 'SentenceDebiasGPT2LMHeadModel' --model_name_or_path 'gpt2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-GPT2Model_c-gpt2_t-gender.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasGPT2LMHeadModel\r\n",
      " - model_name_or_path: gpt2\r\n",
      " - bias_direction: ../results/subspace/subspace_m-GPT2Model_c-gpt2_t-gender.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: gender\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <|endoftext|>\r\n",
      "torch.Size([1, 1, 50257])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [03:00<00:00, 35.47it/s]\r\n",
      "Executing: python stereoset_debias.py --bias_type 'religion' --model 'SentenceDebiasGPT2LMHeadModel' --model_name_or_path 'gpt2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-GPT2Model_c-gpt2_t-religion.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasGPT2LMHeadModel\r\n",
      " - model_name_or_path: gpt2\r\n",
      " - bias_direction: ../results/subspace/subspace_m-GPT2Model_c-gpt2_t-religion.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: religion\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <|endoftext|>\r\n",
      "torch.Size([1, 1, 50257])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [02:58<00:00, 35.88it/s]\r\n",
      "Executing: python stereoset_debias.py --bias_type 'race-color' --model 'SentenceDebiasGPT2LMHeadModel' --model_name_or_path 'gpt2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-GPT2Model_c-gpt2_t-race-color.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasGPT2LMHeadModel\r\n",
      " - model_name_or_path: gpt2\r\n",
      " - bias_direction: ../results/subspace/subspace_m-GPT2Model_c-gpt2_t-race-color.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: race-color\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <|endoftext|>\r\n",
      "torch.Size([1, 1, 50257])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [02:59<00:00, 35.53it/s]\r\n"
     ]
    }
   ],
   "source": [
    "bias_types = [\n",
    "    \"gender\", \n",
    "    \"religion\", \n",
    "    \"race-color\"\n",
    "]\n",
    "\n",
    "for bias in bias_types:\n",
    "    command = f\"python stereoset_debias.py --bias_type '{bias}' --model 'SentenceDebiasGPT2LMHeadModel' --model_name_or_path 'gpt2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-GPT2Model_c-gpt2_t-{bias}.pt'\"\n",
    "    print(f\"Executing: {command}\")\n",
    "    get_ipython().system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be53ef24217c857a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T09:05:58.132434Z",
     "start_time": "2024-08-03T08:25:39.584510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python stereoset_debias.py --bias_type 'gender' --model 'SentenceDebiasPhi2LMHeadModel' --model_name_or_path 'microsoft/phi-2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-gender.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasPhi2LMHeadModel\r\n",
      " - model_name_or_path: microsoft/phi-2\r\n",
      " - bias_direction: ../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-gender.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: gender\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.29s/it]\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <|endoftext|>\r\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n",
      "torch.Size([1, 1, 51200])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [10:02<00:00, 10.60it/s]\r\n",
      "Executing: python stereoset_debias.py --bias_type 'religion' --model 'SentenceDebiasPhi2LMHeadModel' --model_name_or_path 'microsoft/phi-2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-religion.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasPhi2LMHeadModel\r\n",
      " - model_name_or_path: microsoft/phi-2\r\n",
      " - bias_direction: ../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-religion.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: religion\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.41s/it]\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <|endoftext|>\r\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n",
      "torch.Size([1, 1, 51200])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [14:27<00:00,  7.37it/s]\r\n",
      "Executing: python stereoset_debias.py --bias_type 'race-color' --model 'SentenceDebiasPhi2LMHeadModel' --model_name_or_path 'microsoft/phi-2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-race-color.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasPhi2LMHeadModel\r\n",
      " - model_name_or_path: microsoft/phi-2\r\n",
      " - bias_direction: ../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-race-color.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: race-color\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.44s/it]\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <|endoftext|>\r\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n",
      "torch.Size([1, 1, 51200])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [14:49<00:00,  7.18it/s]\r\n"
     ]
    }
   ],
   "source": [
    "bias_types = [\n",
    "    \"gender\", \n",
    "    \"religion\", \n",
    "    \"race-color\"\n",
    "]\n",
    "\n",
    "for bias in bias_types:\n",
    "    command = f\"python stereoset_debias.py --bias_type '{bias}' --model 'SentenceDebiasPhi2LMHeadModel' --model_name_or_path 'microsoft/phi-2' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-PhiForCausalLM_NonBFloat16_c-microsoft_phi-2_t-{bias}.pt'\"\n",
    "    print(f\"Executing: {command}\")\n",
    "    get_ipython().system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b985aa5cca3f00d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T10:41:27.083875Z",
     "start_time": "2024-08-03T09:05:58.133777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python stereoset_debias.py --bias_type 'gender' --model 'SentenceDebiasLlama2LMHeadModel' --model_name_or_path 'meta-llama/Llama-2-7b-hf' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-gender.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasLlama2LMHeadModel\r\n",
      " - model_name_or_path: meta-llama/Llama-2-7b-hf\r\n",
      " - bias_direction: ../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-gender.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: gender\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.35s/it]\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <s>\r\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n",
      "torch.Size([1, 1, 32000])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [31:22<00:00,  3.40it/s]\r\n",
      "Executing: python stereoset_debias.py --bias_type 'religion' --model 'SentenceDebiasLlama2LMHeadModel' --model_name_or_path 'meta-llama/Llama-2-7b-hf' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-religion.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasLlama2LMHeadModel\r\n",
      " - model_name_or_path: meta-llama/Llama-2-7b-hf\r\n",
      " - bias_direction: ../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-religion.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: religion\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.45s/it]\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <s>\r\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n",
      "torch.Size([1, 1, 32000])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [31:22<00:00,  3.40it/s]\r\n",
      "Executing: python stereoset_debias.py --bias_type 'race-color' --model 'SentenceDebiasLlama2LMHeadModel' --model_name_or_path 'meta-llama/Llama-2-7b-hf' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-race-color.pt'\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /home/ongzhiyang/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Running StereoSet:\r\n",
      " - persistent_dir: /home/ongzhiyang/bias-bench-main\r\n",
      " - model: SentenceDebiasLlama2LMHeadModel\r\n",
      " - model_name_or_path: meta-llama/Llama-2-7b-hf\r\n",
      " - bias_direction: ../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-race-color.pt\r\n",
      " - projection_matrix: None\r\n",
      " - load_path: None\r\n",
      " - batch_size: 1\r\n",
      " - bias_type: race-color\r\n",
      " - seed: 42\r\n",
      "Loaded bias direction.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.37s/it]\r\n",
      "Evaluating intrasentence task.\r\n",
      "Unconditional start token: <s>\r\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n",
      "torch.Size([1, 1, 32000])\r\n",
      "100%|███████████████████████████████████████| 6392/6392 [31:24<00:00,  3.39it/s]\r\n"
     ]
    }
   ],
   "source": [
    "bias_types = [\n",
    "    \"gender\", \n",
    "    \"religion\", \n",
    "    \"race-color\"\n",
    "]\n",
    "\n",
    "for bias in bias_types:\n",
    "    command = f\"python stereoset_debias.py --bias_type '{bias}' --model 'SentenceDebiasLlama2LMHeadModel' --model_name_or_path 'meta-llama/Llama-2-7b-hf' --batch_size 1 --seed 42  --bias_direction '../results/subspace/subspace_m-LlamaForCausalLM_NonBFloat16_c-meta-llama_Llama-2-7b-hf_t-{bias}.pt'\"\n",
    "    print(f\"Executing: {command}\")\n",
    "    get_ipython().system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95589551ff49c934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fyp] *",
   "language": "python",
   "name": "conda-env-fyp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
