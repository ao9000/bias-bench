{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36190bd31950e8c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T13:19:56.839077Z",
     "start_time": "2024-08-08T13:19:56.167039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  8 21:19:56 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:01:00.0 Off |                  Off |\r\n",
      "| 58%   82C    P2             299W / 300W |   9489MiB / 49140MiB |    100%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:21:00.0 Off |                  Off |\r\n",
      "| 54%   79C    P2             289W / 300W |  10758MiB / 49140MiB |    100%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA RTX A6000               Off | 00000000:41:00.0 Off |                  Off |\r\n",
      "| 45%   72C    P2             248W / 300W |   4746MiB / 49140MiB |     59%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA RTX A6000               Off | 00000000:42:00.0 Off |                  Off |\r\n",
      "| 30%   51C    P2             129W / 300W |   3588MiB / 49140MiB |     31%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A   3743192      C   python                                     4740MiB |\r\n",
      "|    0   N/A  N/A   3743926      C   python                                     4740MiB |\r\n",
      "|    1   N/A  N/A   3773088      C   python                                     3582MiB |\r\n",
      "|    1   N/A  N/A   3774996      C   python                                     3582MiB |\r\n",
      "|    1   N/A  N/A   3784252      C   python                                     3582MiB |\r\n",
      "|    2   N/A  N/A   3780262      C   python                                     4740MiB |\r\n",
      "|    3   N/A  N/A   3765203      C   python                                     3582MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdac505a492e821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T13:20:07.339290Z",
     "start_time": "2024-08-08T13:20:06.627263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m\u001b[37mcil-hydra          \u001b[m  Thu Aug  8 21:20:07 2024  \u001b[1m\u001b[30m535.183.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 82°C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 9489\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m4740M\u001b[m) \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m4740M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 79°C\u001b[m, \u001b[1m\u001b[32m 99 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10758\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m3582M\u001b[m) \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m3582M\u001b[m) \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m3582M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 72°C\u001b[m, \u001b[1m\u001b[32m 56 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 4746\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m4740M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 51°C\u001b[m, \u001b[1m\u001b[32m 32 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3588\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mongzhiyang\u001b[m(\u001b[33m3582M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e668d9d1d46f46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T00:41:05.371813Z",
     "start_time": "2024-07-07T00:41:05.368927Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95bfa208073730",
   "metadata": {},
   "source": [
    "# Generate subset of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8082f20d6a3a591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T14:25:23.098897Z",
     "start_time": "2024-06-20T14:25:13.301395Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42) # Set a random seed for reproducibility\n",
    "\n",
    "dataset_path = \"/home/FYP/on0008an/bias-bench-main/data/wikipedia-10.txt\"\n",
    "sampling_ratio = 0.25\n",
    "sampled_dataset_path = \"/home/FYP/on0008an/bias-bench-main/data/wikipedia-10_sample.txt\"\n",
    "\n",
    "# Read\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sampled_lines = random.sample(lines, int(len(lines) * sampling_ratio))\n",
    "\n",
    "# Write sampled dataset\n",
    "with open(sampled_dataset_path, \"w\") as f:\n",
    "    f.writelines(sampled_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6eb240323fd16d",
   "metadata": {},
   "source": [
    "# Train models with CDA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831751c9c9e54dce",
   "metadata": {},
   "source": [
    "## All possible arguments:\n",
    "- https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "- ModelArguments (In the script)\n",
    "- DataTrainingArguments (In the script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89f50a8f3839ac",
   "metadata": {},
   "source": [
    "## Lora explanation\n",
    "\n",
    "- Instead of updating weights, Lora tracks changes\n",
    "- Changes are tracked in 2 smaller matrices, multiplied to form the same size matrices as the model original weights\n",
    "\n",
    "Parameters:\n",
    "- Rank: Number of columns in the 2 smaller matrices\n",
    "    - As rank increases, more parameters are fine-tuned\n",
    "    - Downstream tasks intrinsically work well with low rank\n",
    "    - However, complex tasks/behavior that contradicts the pre-training dataset may require higher rank\n",
    "- Training all layers of the network is essential to match full-training performance\n",
    "- Alpha: Scaling factor that is applied to the weight changes when adding to original weights, scale factor = Alpha/Rank\n",
    "    - Microsoft Lora paper sets Alpha = 2 * rank\n",
    "    - QLora paper sets Alpha = 1/4 * rank\n",
    "- Dropout: Randomly set a fraction of the weight changes to zero to prevent overfitting\n",
    "    - QLora paper sets dropout = 0.1 for 7B, 13B models, 0.05 for 33B, 65B models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb82739ad9d1a94",
   "metadata": {},
   "source": [
    "# To achieve the most out of Lora fine-tuning, we must train all layers of the network\n",
    "\n",
    "Llama 2:\n",
    "- Linear Layers: gate_proj, down_proj, up_proj, q_proj, v_proj, k_proj, and o_proj\n",
    "\n",
    "#If only targeting attention blocks of the model\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "#If targeting all linear layers\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c9b39d41ff29f",
   "metadata": {},
   "source": [
    "# Following parameters from the batch script\n",
    "\n",
    "- Cache takes around 3 runs to be built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f0337ae517d4a",
   "metadata": {},
   "source": [
    "## For GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e07dc1bc0b0baf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-07T00:41:17.509576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ongzhiyang/anaconda3/envs/fyp/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "07/07/2024 08:41:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\r\n",
      "Padding token: <|endoftext|>\r\n",
      "Quantization enabled\r\n",
      "Training target modules: ['query', 'value']\r\n",
      "Applying counterfactual augmentation:  25%|▏| 1322000/5349612 [34:58<1:50:10, 60"
     ]
    }
   ],
   "source": [
    "!python run_clm.py --model_name_or_path \"gpt2\" --tokenizer_name \"gpt2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 42 --output_dir \"../results/CDA_FT/gpt2/gender\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad88d5930f3137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py --model_name_or_path \"gpt2\" --tokenizer_name \"gpt2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"race\" --seed 42 --output_dir \"../results/CDA_FT/gpt2/race\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fba1e03c0ec29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py --model_name_or_path \"gpt2\" --tokenizer_name \"gpt2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"religion\" --seed 42 --output_dir \"../results/CDA_FT/gpt2/religion\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc603cf1afac9153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T15:21:07.215151Z",
     "start_time": "2024-06-13T15:19:54.802531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "06/13/2024 23:20:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "06/13/2024 23:20:20 - WARNING - accelerate.utils.other - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\r\n",
      "[WARNING|trainer.py:591] 2024-06-13 23:20:20,747 >> max_steps is given, it will override any value given in num_train_epochs\r\n",
      "{'loss': 6.7748, 'grad_norm': 104.59912872314453, 'learning_rate': 4.99e-05, 'epoch': 0.01}\r\n",
      "  0%|                                         | 1/500 [00:30<4:12:31, 30.36s/it]^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/FYP/on0008an/bias-bench-main/experiments/run_clm.py\", line 790, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/FYP/on0008an/bias-bench-main/experiments/run_clm.py\", line 729, in main\r\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/trainer.py\", line 1885, in train\r\n",
      "    return inner_training_loop(\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/trainer.py\", line 2216, in _inner_training_loop\r\n",
      "    tr_loss_step = self.training_step(model, inputs)\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/trainer.py\", line 3241, in training_step\r\n",
      "    torch.cuda.empty_cache()\r\n",
      "  File \"/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/torch/cuda/memory.py\", line 162, in empty_cache\r\n",
      "    torch._C._cuda_emptyCache()\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "# 500 max_steps to fit into 6hours training timeframe\n",
    "!python run_clm.py --model_name_or_path \"gpt2\" --do_train --train_file \"../data/wikipedia-10_tiny.txt\" --max_steps 500 --per_device_train_batch_size 8 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 0 --output_dir \"../results/CDA_FT/gpt2\" --persistent_dir \"/home/FYP/on0008an/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e397262e82ce819",
   "metadata": {},
   "source": [
    "## To continue training from a checkpoint, need to set steps to the number of steps already done + what you want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49230b93dd58c4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T06:54:52.972170Z",
     "start_time": "2024-06-12T02:32:28.277659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "06/12/2024 10:32:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\r\n",
      "06/12/2024 10:32:37 - WARNING - datasets.builder - Using custom data configuration default-8f3a313a0598a372\r\n",
      "06/12/2024 10:32:37 - WARNING - datasets.builder - Reusing dataset text (/home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 67.07it/s]\r\n",
      "06/12/2024 10:32:38 - WARNING - datasets.builder - Using custom data configuration default-8f3a313a0598a372\r\n",
      "06/12/2024 10:32:38 - WARNING - datasets.builder - Reusing dataset text (/home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\r\n",
      "06/12/2024 10:32:38 - WARNING - datasets.builder - Using custom data configuration default-8f3a313a0598a372\r\n",
      "06/12/2024 10:32:38 - WARNING - datasets.builder - Reusing dataset text (/home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x2aab860a5630> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-629f6fbed82c07cd.arrow\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-e3e70682c2094cac.arrow\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-0a5d2f346baa9455.arrow\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-f728b4fa42485e3a.arrow\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-7c65c1e582e2e662.arrow\r\n",
      "06/12/2024 10:32:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/FYP/on0008an/.cache/huggingface/datasets/text/default-8f3a313a0598a372/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-eb1167b367a9c378.arrow\r\n",
      "06/12/2024 10:32:40 - WARNING - accelerate.utils.other - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\r\n",
      "[WARNING|trainer.py:591] 2024-06-12 10:32:41,170 >> max_steps is given, it will override any value given in num_train_epochs\r\n",
      "[WARNING|trainer.py:2689] 2024-06-12 10:32:41,453 >> There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\r\n",
      "{'loss': 0.6557, 'grad_norm': 0.2115873247385025, 'learning_rate': 2.495e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6238, 'grad_norm': 0.19264155626296997, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.636, 'grad_norm': 0.3111065924167633, 'learning_rate': 2.485e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6642, 'grad_norm': 0.3562127351760864, 'learning_rate': 2.48e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6896, 'grad_norm': 0.30045801401138306, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6368, 'grad_norm': 0.35026121139526367, 'learning_rate': 2.47e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6551, 'grad_norm': 0.606468141078949, 'learning_rate': 2.465e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6384, 'grad_norm': 0.4467180371284485, 'learning_rate': 2.46e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6552, 'grad_norm': 0.3130481243133545, 'learning_rate': 2.455e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6287, 'grad_norm': 0.44069036841392517, 'learning_rate': 2.45e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.6347, 'grad_norm': 0.34402206540107727, 'learning_rate': 2.445e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6786, 'grad_norm': 0.20935572683811188, 'learning_rate': 2.44e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6292, 'grad_norm': 0.3501598834991455, 'learning_rate': 2.435e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6551, 'grad_norm': 0.45371097326278687, 'learning_rate': 2.43e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6544, 'grad_norm': 0.20418870449066162, 'learning_rate': 2.425e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.618, 'grad_norm': 0.39605623483657837, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6842, 'grad_norm': 0.2552267611026764, 'learning_rate': 2.415e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6634, 'grad_norm': 0.26863980293273926, 'learning_rate': 2.41e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6529, 'grad_norm': 0.2597677409648895, 'learning_rate': 2.4050000000000002e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6223, 'grad_norm': 0.27123522758483887, 'learning_rate': 2.4e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.667, 'grad_norm': 0.23355042934417725, 'learning_rate': 2.395e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.689, 'grad_norm': 0.2857970595359802, 'learning_rate': 2.39e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.61, 'grad_norm': 0.28012046217918396, 'learning_rate': 2.385e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6534, 'grad_norm': 0.27622461318969727, 'learning_rate': 2.38e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6531, 'grad_norm': 0.23893120884895325, 'learning_rate': 2.375e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6347, 'grad_norm': 0.458604097366333, 'learning_rate': 2.37e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6288, 'grad_norm': 0.2688034176826477, 'learning_rate': 2.365e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6208, 'grad_norm': 0.3383215069770813, 'learning_rate': 2.36e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6445, 'grad_norm': 0.26253020763397217, 'learning_rate': 2.355e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6433, 'grad_norm': 0.4139745831489563, 'learning_rate': 2.35e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6605, 'grad_norm': 0.2078595757484436, 'learning_rate': 2.345e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6254, 'grad_norm': 0.3230346441268921, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6204, 'grad_norm': 0.4501117467880249, 'learning_rate': 2.3350000000000002e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6798, 'grad_norm': 0.25581079721450806, 'learning_rate': 2.3300000000000004e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6556, 'grad_norm': 0.29716774821281433, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6704, 'grad_norm': 0.26099270582199097, 'learning_rate': 2.32e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6035, 'grad_norm': 0.43177348375320435, 'learning_rate': 2.3150000000000004e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6358, 'grad_norm': 0.5854957699775696, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.5948, 'grad_norm': 0.3273097276687622, 'learning_rate': 2.305e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6086, 'grad_norm': 0.23514391481876373, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6478, 'grad_norm': 0.5205389261245728, 'learning_rate': 2.2950000000000002e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6424, 'grad_norm': 0.40519648790359497, 'learning_rate': 2.29e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6176, 'grad_norm': 0.5347437858581543, 'learning_rate': 2.2850000000000003e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6523, 'grad_norm': 0.2758011817932129, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6567, 'grad_norm': 0.30242857336997986, 'learning_rate': 2.275e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.652, 'grad_norm': 0.3763255178928375, 'learning_rate': 2.2700000000000003e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.5864, 'grad_norm': 0.5666506290435791, 'learning_rate': 2.265e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6267, 'grad_norm': 0.4683898389339447, 'learning_rate': 2.26e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.623, 'grad_norm': 0.40777409076690674, 'learning_rate': 2.2550000000000003e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6559, 'grad_norm': 0.3417452275753021, 'learning_rate': 2.25e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.6742, 'grad_norm': 0.6755599975585938, 'learning_rate': 2.245e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.5745, 'grad_norm': 0.31666669249534607, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6745, 'grad_norm': 0.8507686853408813, 'learning_rate': 2.235e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.5858, 'grad_norm': 0.8594740629196167, 'learning_rate': 2.23e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6662, 'grad_norm': 0.31526872515678406, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6604, 'grad_norm': 0.7714316844940186, 'learning_rate': 2.22e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6649, 'grad_norm': 0.46568822860717773, 'learning_rate': 2.215e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6368, 'grad_norm': 0.31908348202705383, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6778, 'grad_norm': 0.35507094860076904, 'learning_rate': 2.205e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6927, 'grad_norm': 0.46510595083236694, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6236, 'grad_norm': 0.30375224351882935, 'learning_rate': 2.195e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6641, 'grad_norm': 0.31649768352508545, 'learning_rate': 2.19e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6257, 'grad_norm': 0.35538986325263977, 'learning_rate': 2.1850000000000003e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6466, 'grad_norm': 0.40376171469688416, 'learning_rate': 2.18e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6681, 'grad_norm': 0.4744826555252075, 'learning_rate': 2.175e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6199, 'grad_norm': 0.30352306365966797, 'learning_rate': 2.1700000000000002e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6761, 'grad_norm': 0.4745550751686096, 'learning_rate': 2.165e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6584, 'grad_norm': 0.4987163245677948, 'learning_rate': 2.16e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6646, 'grad_norm': 0.29962027072906494, 'learning_rate': 2.1550000000000002e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6493, 'grad_norm': 0.5500877499580383, 'learning_rate': 2.15e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6141, 'grad_norm': 0.38703396916389465, 'learning_rate': 2.145e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6203, 'grad_norm': 0.3045256733894348, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.5922, 'grad_norm': 0.4787152409553528, 'learning_rate': 2.135e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6128, 'grad_norm': 0.7406815886497498, 'learning_rate': 2.13e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6804, 'grad_norm': 0.5350363850593567, 'learning_rate': 2.125e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6863, 'grad_norm': 0.3807249665260315, 'learning_rate': 2.12e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6887, 'grad_norm': 0.32376062870025635, 'learning_rate': 2.115e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6404, 'grad_norm': 0.3960086405277252, 'learning_rate': 2.11e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6423, 'grad_norm': 0.6549795866012573, 'learning_rate': 2.105e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6826, 'grad_norm': 0.3324780762195587, 'learning_rate': 2.1e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6407, 'grad_norm': 0.5013861060142517, 'learning_rate': 2.095e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6361, 'grad_norm': 0.2873983681201935, 'learning_rate': 2.09e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6583, 'grad_norm': 0.4758370816707611, 'learning_rate': 2.085e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6428, 'grad_norm': 0.362221896648407, 'learning_rate': 2.08e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6042, 'grad_norm': 0.8137452602386475, 'learning_rate': 2.075e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.651, 'grad_norm': 0.3966907560825348, 'learning_rate': 2.07e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6902, 'grad_norm': 0.5383843183517456, 'learning_rate': 2.065e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.611, 'grad_norm': 0.6459275484085083, 'learning_rate': 2.06e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6379, 'grad_norm': 0.7329563498497009, 'learning_rate': 2.055e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6625, 'grad_norm': 0.8898820877075195, 'learning_rate': 2.05e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6631, 'grad_norm': 0.40498819947242737, 'learning_rate': 2.045e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6147, 'grad_norm': 0.3773667812347412, 'learning_rate': 2.04e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.6426, 'grad_norm': 0.5107411742210388, 'learning_rate': 2.035e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6586, 'grad_norm': 0.4576224982738495, 'learning_rate': 2.0300000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6474, 'grad_norm': 0.32808917760849, 'learning_rate': 2.025e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6429, 'grad_norm': 0.6894828677177429, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6359, 'grad_norm': 0.4295075833797455, 'learning_rate': 2.0150000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6785, 'grad_norm': 0.5515010952949524, 'learning_rate': 2.01e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6433, 'grad_norm': 1.0046215057373047, 'learning_rate': 2.0050000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6554, 'grad_norm': 0.7092289924621582, 'learning_rate': 2e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6536, 'grad_norm': 0.5174861550331116, 'learning_rate': 1.995e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6333, 'grad_norm': 0.7729364037513733, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6551, 'grad_norm': 0.5503351092338562, 'learning_rate': 1.985e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.666, 'grad_norm': 0.6434793472290039, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6461, 'grad_norm': 0.7080270648002625, 'learning_rate': 1.9750000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6294, 'grad_norm': 1.2003668546676636, 'learning_rate': 1.97e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6112, 'grad_norm': 0.8876971006393433, 'learning_rate': 1.9650000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6719, 'grad_norm': 1.1498562097549438, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.673, 'grad_norm': 1.470005989074707, 'learning_rate': 1.955e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6499, 'grad_norm': 1.8935884237289429, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6753, 'grad_norm': 2.236222982406616, 'learning_rate': 1.9450000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6508, 'grad_norm': 0.917537271976471, 'learning_rate': 1.94e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6032, 'grad_norm': 0.6615732908248901, 'learning_rate': 1.9350000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6455, 'grad_norm': 0.5955935120582581, 'learning_rate': 1.93e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6332, 'grad_norm': 0.5345644354820251, 'learning_rate': 1.925e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6672, 'grad_norm': 0.42181167006492615, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6295, 'grad_norm': 1.5720123052597046, 'learning_rate': 1.915e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6069, 'grad_norm': 1.150220513343811, 'learning_rate': 1.91e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6268, 'grad_norm': 0.5301508903503418, 'learning_rate': 1.9050000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6624, 'grad_norm': 1.3472484350204468, 'learning_rate': 1.9e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6493, 'grad_norm': 1.2281320095062256, 'learning_rate': 1.895e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6081, 'grad_norm': 0.8384655117988586, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6377, 'grad_norm': 0.4625558853149414, 'learning_rate': 1.885e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6577, 'grad_norm': 0.5842968225479126, 'learning_rate': 1.88e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6084, 'grad_norm': 0.6160885095596313, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6558, 'grad_norm': 0.6359966993331909, 'learning_rate': 1.87e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6537, 'grad_norm': 0.856984555721283, 'learning_rate': 1.865e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6404, 'grad_norm': 0.7286372184753418, 'learning_rate': 1.86e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6309, 'grad_norm': 0.45259273052215576, 'learning_rate': 1.855e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6037, 'grad_norm': 1.2452492713928223, 'learning_rate': 1.85e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.7017, 'grad_norm': 1.2458356618881226, 'learning_rate': 1.845e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.672, 'grad_norm': 3.6752586364746094, 'learning_rate': 1.84e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.6644, 'grad_norm': 2.290172815322876, 'learning_rate': 1.8350000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6292, 'grad_norm': 1.2933751344680786, 'learning_rate': 1.83e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6206, 'grad_norm': 1.033409833908081, 'learning_rate': 1.825e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6203, 'grad_norm': 2.383342981338501, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6808, 'grad_norm': 2.3166229724884033, 'learning_rate': 1.815e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6255, 'grad_norm': 1.9762972593307495, 'learning_rate': 1.81e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6902, 'grad_norm': 1.6428489685058594, 'learning_rate': 1.805e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6456, 'grad_norm': 1.607475757598877, 'learning_rate': 1.8e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6346, 'grad_norm': 1.3517221212387085, 'learning_rate': 1.795e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.678, 'grad_norm': 0.5698007345199585, 'learning_rate': 1.79e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6147, 'grad_norm': 0.5662368535995483, 'learning_rate': 1.785e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6272, 'grad_norm': 0.7701658010482788, 'learning_rate': 1.78e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.688, 'grad_norm': 0.7572707533836365, 'learning_rate': 1.775e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6909, 'grad_norm': 0.563847541809082, 'learning_rate': 1.77e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.7074, 'grad_norm': 0.3247334659099579, 'learning_rate': 1.765e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6516, 'grad_norm': 0.5919867753982544, 'learning_rate': 1.76e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6462, 'grad_norm': 0.652765154838562, 'learning_rate': 1.755e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6578, 'grad_norm': 1.1712762117385864, 'learning_rate': 1.75e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6468, 'grad_norm': 1.1853095293045044, 'learning_rate': 1.745e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6452, 'grad_norm': 1.0197486877441406, 'learning_rate': 1.74e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6339, 'grad_norm': 1.7846908569335938, 'learning_rate': 1.7349999999999998e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6119, 'grad_norm': 1.0749787092208862, 'learning_rate': 1.73e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6346, 'grad_norm': 0.8823636770248413, 'learning_rate': 1.725e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6406, 'grad_norm': 0.7538008689880371, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6294, 'grad_norm': 1.0350430011749268, 'learning_rate': 1.7150000000000004e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.7066, 'grad_norm': 0.7023168802261353, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6255, 'grad_norm': 0.9334840178489685, 'learning_rate': 1.705e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6276, 'grad_norm': 1.220866084098816, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6821, 'grad_norm': 1.379399061203003, 'learning_rate': 1.6950000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6151, 'grad_norm': 0.9968558549880981, 'learning_rate': 1.69e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6416, 'grad_norm': 1.5052270889282227, 'learning_rate': 1.6850000000000003e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6798, 'grad_norm': 1.362597942352295, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6319, 'grad_norm': 2.3488852977752686, 'learning_rate': 1.675e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6329, 'grad_norm': 1.4678510427474976, 'learning_rate': 1.6700000000000003e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6127, 'grad_norm': 1.5164357423782349, 'learning_rate': 1.665e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.5984, 'grad_norm': 1.4047718048095703, 'learning_rate': 1.66e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6685, 'grad_norm': 1.3697996139526367, 'learning_rate': 1.6550000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6714, 'grad_norm': 1.1094582080841064, 'learning_rate': 1.65e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6218, 'grad_norm': 1.5714733600616455, 'learning_rate': 1.645e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6507, 'grad_norm': 1.1422500610351562, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6936, 'grad_norm': 0.9161373376846313, 'learning_rate': 1.635e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.6191, 'grad_norm': 1.0088962316513062, 'learning_rate': 1.63e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6429, 'grad_norm': 1.5608938932418823, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.574, 'grad_norm': 2.7820441722869873, 'learning_rate': 1.62e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6449, 'grad_norm': 1.539480447769165, 'learning_rate': 1.6150000000000003e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6219, 'grad_norm': 1.206963300704956, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6501, 'grad_norm': 0.7670510411262512, 'learning_rate': 1.605e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6494, 'grad_norm': 2.84438419342041, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6172, 'grad_norm': 2.068002223968506, 'learning_rate': 1.595e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6394, 'grad_norm': 3.114628553390503, 'learning_rate': 1.59e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6928, 'grad_norm': 6.3131208419799805, 'learning_rate': 1.5850000000000002e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6303, 'grad_norm': 2.1739912033081055, 'learning_rate': 1.58e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6245, 'grad_norm': 1.857779622077942, 'learning_rate': 1.575e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.7126, 'grad_norm': 0.5491353869438171, 'learning_rate': 1.5700000000000002e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6684, 'grad_norm': 0.4970249831676483, 'learning_rate': 1.565e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.633, 'grad_norm': 0.4187225103378296, 'learning_rate': 1.56e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6313, 'grad_norm': 0.5590975284576416, 'learning_rate': 1.5550000000000002e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6277, 'grad_norm': 0.6109511256217957, 'learning_rate': 1.55e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6203, 'grad_norm': 0.5801092386245728, 'learning_rate': 1.545e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6221, 'grad_norm': 0.3035666346549988, 'learning_rate': 1.54e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6489, 'grad_norm': 0.32755276560783386, 'learning_rate': 1.535e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6071, 'grad_norm': 0.31512078642845154, 'learning_rate': 1.53e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6396, 'grad_norm': 0.34198421239852905, 'learning_rate': 1.525e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.594, 'grad_norm': 0.359731525182724, 'learning_rate': 1.52e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6209, 'grad_norm': 0.7421799302101135, 'learning_rate': 1.515e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.7065, 'grad_norm': 0.4842282235622406, 'learning_rate': 1.51e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6554, 'grad_norm': 1.1700948476791382, 'learning_rate': 1.505e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6684, 'grad_norm': 1.7746645212173462, 'learning_rate': 1.5e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6668, 'grad_norm': 0.7756468057632446, 'learning_rate': 1.4950000000000001e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6703, 'grad_norm': 1.014446496963501, 'learning_rate': 1.49e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6309, 'grad_norm': 0.7164064049720764, 'learning_rate': 1.485e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6445, 'grad_norm': 0.6639392375946045, 'learning_rate': 1.48e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6398, 'grad_norm': 1.0222387313842773, 'learning_rate': 1.475e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6551, 'grad_norm': 1.3042017221450806, 'learning_rate': 1.47e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6244, 'grad_norm': 2.4191834926605225, 'learning_rate': 1.465e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6548, 'grad_norm': 3.2128102779388428, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.677, 'grad_norm': 7.089823246002197, 'learning_rate': 1.455e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6402, 'grad_norm': 6.820611000061035, 'learning_rate': 1.45e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6847, 'grad_norm': 6.1055073738098145, 'learning_rate': 1.4449999999999999e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6713, 'grad_norm': 6.651617050170898, 'learning_rate': 1.44e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6253, 'grad_norm': 8.315253257751465, 'learning_rate': 1.435e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.651, 'grad_norm': 9.498734474182129, 'learning_rate': 1.43e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.6249, 'grad_norm': 10.950319290161133, 'learning_rate': 1.4249999999999999e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6449, 'grad_norm': 8.9098482131958, 'learning_rate': 1.42e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6257, 'grad_norm': 4.508763790130615, 'learning_rate': 1.415e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6221, 'grad_norm': 4.9165825843811035, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6226, 'grad_norm': 5.393686294555664, 'learning_rate': 1.4050000000000003e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6619, 'grad_norm': 2.478397846221924, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6852, 'grad_norm': 2.9820356369018555, 'learning_rate': 1.3950000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.657, 'grad_norm': 3.7294557094573975, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6219, 'grad_norm': 1.9512079954147339, 'learning_rate': 1.3850000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6719, 'grad_norm': 2.7286813259124756, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.68, 'grad_norm': 3.3751962184906006, 'learning_rate': 1.3750000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6723, 'grad_norm': 4.0280680656433105, 'learning_rate': 1.3700000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6471, 'grad_norm': 2.9363441467285156, 'learning_rate': 1.3650000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6628, 'grad_norm': 2.7786378860473633, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.662, 'grad_norm': 1.6338090896606445, 'learning_rate': 1.3550000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6922, 'grad_norm': 1.5894287824630737, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6699, 'grad_norm': 1.384964108467102, 'learning_rate': 1.3450000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6251, 'grad_norm': 3.2503812313079834, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6163, 'grad_norm': 1.821259617805481, 'learning_rate': 1.3350000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6165, 'grad_norm': 3.343188762664795, 'learning_rate': 1.3300000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6637, 'grad_norm': 6.411457061767578, 'learning_rate': 1.3250000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6517, 'grad_norm': 4.120389938354492, 'learning_rate': 1.32e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6326, 'grad_norm': 1.8982994556427002, 'learning_rate': 1.3150000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.66, 'grad_norm': 2.164191484451294, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.647, 'grad_norm': 1.5809420347213745, 'learning_rate': 1.305e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6118, 'grad_norm': 1.2991682291030884, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6192, 'grad_norm': 2.2070045471191406, 'learning_rate': 1.2950000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6505, 'grad_norm': 0.9022579789161682, 'learning_rate': 1.29e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6528, 'grad_norm': 1.694169521331787, 'learning_rate': 1.285e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.7033, 'grad_norm': 2.4958839416503906, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6618, 'grad_norm': 2.7213165760040283, 'learning_rate': 1.2750000000000002e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6197, 'grad_norm': 2.406888246536255, 'learning_rate': 1.27e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6772, 'grad_norm': 2.1198174953460693, 'learning_rate': 1.2650000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.641, 'grad_norm': 5.089433193206787, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.651, 'grad_norm': 4.872777938842773, 'learning_rate': 1.255e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6789, 'grad_norm': 4.659510135650635, 'learning_rate': 1.25e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6542, 'grad_norm': 1.9805606603622437, 'learning_rate': 1.2450000000000001e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6607, 'grad_norm': 3.645439863204956, 'learning_rate': 1.24e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6487, 'grad_norm': 1.7407666444778442, 'learning_rate': 1.235e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6421, 'grad_norm': 1.9810590744018555, 'learning_rate': 1.23e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6434, 'grad_norm': 1.3513044118881226, 'learning_rate': 1.225e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.6621, 'grad_norm': 3.0073368549346924, 'learning_rate': 1.22e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.5974, 'grad_norm': 3.957909345626831, 'learning_rate': 1.215e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6542, 'grad_norm': 2.313876152038574, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6347, 'grad_norm': 2.6929028034210205, 'learning_rate': 1.205e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6595, 'grad_norm': 1.839199185371399, 'learning_rate': 1.2e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6021, 'grad_norm': 7.874794960021973, 'learning_rate': 1.195e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6676, 'grad_norm': 3.9770097732543945, 'learning_rate': 1.19e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6687, 'grad_norm': 2.476727247238159, 'learning_rate': 1.185e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.7008, 'grad_norm': 4.301414489746094, 'learning_rate': 1.18e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6231, 'grad_norm': 2.3125667572021484, 'learning_rate': 1.175e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.61, 'grad_norm': 3.639707088470459, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6122, 'grad_norm': 2.476952314376831, 'learning_rate': 1.1650000000000002e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6545, 'grad_norm': 3.7448747158050537, 'learning_rate': 1.16e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.629, 'grad_norm': 4.168390274047852, 'learning_rate': 1.1550000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6503, 'grad_norm': 1.1903074979782104, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6623, 'grad_norm': 5.120424270629883, 'learning_rate': 1.145e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.663, 'grad_norm': 2.334918975830078, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6534, 'grad_norm': 3.3273394107818604, 'learning_rate': 1.1350000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6446, 'grad_norm': 2.797818660736084, 'learning_rate': 1.13e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6098, 'grad_norm': 2.363332748413086, 'learning_rate': 1.125e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6392, 'grad_norm': 1.209658145904541, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6311, 'grad_norm': 2.5815091133117676, 'learning_rate': 1.115e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6688, 'grad_norm': 1.577420711517334, 'learning_rate': 1.11e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6449, 'grad_norm': 2.9259002208709717, 'learning_rate': 1.1050000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6637, 'grad_norm': 1.9868425130844116, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6456, 'grad_norm': 1.6785697937011719, 'learning_rate': 1.095e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6489, 'grad_norm': 2.438558578491211, 'learning_rate': 1.09e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.66, 'grad_norm': 2.5754899978637695, 'learning_rate': 1.0850000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6256, 'grad_norm': 1.6525863409042358, 'learning_rate': 1.08e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6625, 'grad_norm': 3.663297176361084, 'learning_rate': 1.075e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6041, 'grad_norm': 1.1589049100875854, 'learning_rate': 1.0700000000000001e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6784, 'grad_norm': 2.9014413356781006, 'learning_rate': 1.065e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6455, 'grad_norm': 3.06840443611145, 'learning_rate': 1.06e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.5827, 'grad_norm': 1.2938027381896973, 'learning_rate': 1.055e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6137, 'grad_norm': 2.861983060836792, 'learning_rate': 1.05e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6062, 'grad_norm': 2.435669183731079, 'learning_rate': 1.045e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6371, 'grad_norm': 2.482912302017212, 'learning_rate': 1.04e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6544, 'grad_norm': 1.5743927955627441, 'learning_rate': 1.035e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6776, 'grad_norm': 2.3458940982818604, 'learning_rate': 1.03e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6426, 'grad_norm': 1.4601761102676392, 'learning_rate': 1.025e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6643, 'grad_norm': 2.6948697566986084, 'learning_rate': 1.02e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.6357, 'grad_norm': 1.6481726169586182, 'learning_rate': 1.0150000000000001e-05, 'epoch': 0.2}\r\n",
      "{'loss': 0.6586, 'grad_norm': 3.029620409011841, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.2}\r\n",
      "{'loss': 0.6284, 'grad_norm': 2.8712377548217773, 'learning_rate': 1.005e-05, 'epoch': 0.2}\r\n",
      "{'loss': 0.6149, 'grad_norm': 2.452873468399048, 'learning_rate': 1e-05, 'epoch': 0.2}\r\n",
      "{'loss': 0.635, 'grad_norm': 3.4158012866973877, 'learning_rate': 9.950000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6507, 'grad_norm': 4.985315322875977, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6247, 'grad_norm': 6.844456672668457, 'learning_rate': 9.85e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6339, 'grad_norm': 2.016141653060913, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6763, 'grad_norm': 3.463444709777832, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6615, 'grad_norm': 2.628861904144287, 'learning_rate': 9.7e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6646, 'grad_norm': 2.578906297683716, 'learning_rate': 9.65e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.685, 'grad_norm': 3.073453664779663, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6982, 'grad_norm': 6.056844711303711, 'learning_rate': 9.55e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6864, 'grad_norm': 4.331027030944824, 'learning_rate': 9.5e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6807, 'grad_norm': 2.9973948001861572, 'learning_rate': 9.450000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6316, 'grad_norm': 7.516840934753418, 'learning_rate': 9.4e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6582, 'grad_norm': 6.41497802734375, 'learning_rate': 9.35e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6414, 'grad_norm': 6.851447582244873, 'learning_rate': 9.3e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6621, 'grad_norm': 6.300727367401123, 'learning_rate': 9.25e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6587, 'grad_norm': 3.625694751739502, 'learning_rate': 9.2e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6438, 'grad_norm': 2.9107484817504883, 'learning_rate': 9.15e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6652, 'grad_norm': 2.426358222961426, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6172, 'grad_norm': 8.726700782775879, 'learning_rate': 9.05e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6671, 'grad_norm': 4.092419147491455, 'learning_rate': 9e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6619, 'grad_norm': 3.7388126850128174, 'learning_rate': 8.95e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6597, 'grad_norm': 3.1543686389923096, 'learning_rate': 8.9e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6605, 'grad_norm': 2.003295660018921, 'learning_rate': 8.85e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6516, 'grad_norm': 5.340728759765625, 'learning_rate': 8.8e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6428, 'grad_norm': 1.9233888387680054, 'learning_rate': 8.75e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6745, 'grad_norm': 2.3421573638916016, 'learning_rate': 8.7e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6533, 'grad_norm': 2.995185375213623, 'learning_rate': 8.65e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.5907, 'grad_norm': 2.852820873260498, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6458, 'grad_norm': 4.063917636871338, 'learning_rate': 8.550000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.644, 'grad_norm': 2.7187204360961914, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6436, 'grad_norm': 2.481830358505249, 'learning_rate': 8.45e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6407, 'grad_norm': 3.7787628173828125, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6728, 'grad_norm': 2.405512571334839, 'learning_rate': 8.350000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6318, 'grad_norm': 3.2897071838378906, 'learning_rate': 8.3e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6223, 'grad_norm': 3.8386712074279785, 'learning_rate': 8.25e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6561, 'grad_norm': 2.236079692840576, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6774, 'grad_norm': 2.8347232341766357, 'learning_rate': 8.15e-06, 'epoch': 0.2}\r\n",
      "{'loss': 0.6881, 'grad_norm': 2.0973901748657227, 'learning_rate': 8.1e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6605, 'grad_norm': 3.508617877960205, 'learning_rate': 8.050000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6349, 'grad_norm': 1.5219480991363525, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6756, 'grad_norm': 0.9902554154396057, 'learning_rate': 7.95e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6823, 'grad_norm': 1.4460123777389526, 'learning_rate': 7.9e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6396, 'grad_norm': 2.1245808601379395, 'learning_rate': 7.850000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6585, 'grad_norm': 1.219173789024353, 'learning_rate': 7.8e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6076, 'grad_norm': 2.0414531230926514, 'learning_rate': 7.75e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6513, 'grad_norm': 2.351304292678833, 'learning_rate': 7.7e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6186, 'grad_norm': 2.3595681190490723, 'learning_rate': 7.65e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6382, 'grad_norm': 4.976821422576904, 'learning_rate': 7.6e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6347, 'grad_norm': 10.77099323272705, 'learning_rate': 7.55e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6231, 'grad_norm': 3.769301176071167, 'learning_rate': 7.5e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.662, 'grad_norm': 7.027824401855469, 'learning_rate': 7.45e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6825, 'grad_norm': 16.529455184936523, 'learning_rate': 7.4e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6345, 'grad_norm': 9.111929893493652, 'learning_rate': 7.35e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6593, 'grad_norm': 17.226533889770508, 'learning_rate': 7.2999999999999996e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6593, 'grad_norm': 22.83696174621582, 'learning_rate': 7.25e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6177, 'grad_norm': 35.4556999206543, 'learning_rate': 7.2e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6594, 'grad_norm': 34.373870849609375, 'learning_rate': 7.15e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6283, 'grad_norm': 21.296655654907227, 'learning_rate': 7.1e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6528, 'grad_norm': 60.36719512939453, 'learning_rate': 7.049999999999999e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6306, 'grad_norm': 41.54646682739258, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6666, 'grad_norm': 41.65903091430664, 'learning_rate': 6.950000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6901, 'grad_norm': 30.945444107055664, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6519, 'grad_norm': 53.785438537597656, 'learning_rate': 6.8500000000000005e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6376, 'grad_norm': 58.049766540527344, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6544, 'grad_norm': 40.0172119140625, 'learning_rate': 6.750000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6471, 'grad_norm': 33.8600959777832, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6462, 'grad_norm': 31.130573272705078, 'learning_rate': 6.650000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6472, 'grad_norm': 34.238216400146484, 'learning_rate': 6.6e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.7031, 'grad_norm': 33.6507682800293, 'learning_rate': 6.550000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6663, 'grad_norm': 35.11989212036133, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6634, 'grad_norm': 19.669082641601562, 'learning_rate': 6.45e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6415, 'grad_norm': 29.8083438873291, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6336, 'grad_norm': 27.811307907104492, 'learning_rate': 6.35e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6271, 'grad_norm': 22.244966506958008, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6778, 'grad_norm': 17.172048568725586, 'learning_rate': 6.25e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6409, 'grad_norm': 12.925156593322754, 'learning_rate': 6.2e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6491, 'grad_norm': 23.225906372070312, 'learning_rate': 6.15e-06, 'epoch': 0.21}\r\n",
      "{'loss': 0.6717, 'grad_norm': 16.816137313842773, 'learning_rate': 6.1e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6485, 'grad_norm': 27.13277244567871, 'learning_rate': 6.0500000000000005e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6908, 'grad_norm': 16.54088020324707, 'learning_rate': 6e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6517, 'grad_norm': 12.483222007751465, 'learning_rate': 5.95e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6451, 'grad_norm': 6.36439323425293, 'learning_rate': 5.9e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6728, 'grad_norm': 9.693863868713379, 'learning_rate': 5.850000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6349, 'grad_norm': 3.9128189086914062, 'learning_rate': 5.8e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6671, 'grad_norm': 5.624101638793945, 'learning_rate': 5.750000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6373, 'grad_norm': 5.176430702209473, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6674, 'grad_norm': 2.9736716747283936, 'learning_rate': 5.65e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6662, 'grad_norm': 4.045880317687988, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6035, 'grad_norm': 5.288593769073486, 'learning_rate': 5.55e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6768, 'grad_norm': 3.078551769256592, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6525, 'grad_norm': 3.3812172412872314, 'learning_rate': 5.45e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6503, 'grad_norm': 5.33840274810791, 'learning_rate': 5.4e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6913, 'grad_norm': 3.292956590652466, 'learning_rate': 5.3500000000000004e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6721, 'grad_norm': 4.71843147277832, 'learning_rate': 5.3e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.7046, 'grad_norm': 5.047091484069824, 'learning_rate': 5.25e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.7645, 'grad_norm': 4.1756134033203125, 'learning_rate': 5.2e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6451, 'grad_norm': 4.25787353515625, 'learning_rate': 5.15e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6547, 'grad_norm': 4.327764511108398, 'learning_rate': 5.1e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6399, 'grad_norm': 3.2226269245147705, 'learning_rate': 5.050000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6763, 'grad_norm': 3.4376626014709473, 'learning_rate': 5e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6827, 'grad_norm': 4.156784534454346, 'learning_rate': 4.950000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6544, 'grad_norm': 3.313734531402588, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6207, 'grad_norm': 3.5048797130584717, 'learning_rate': 4.85e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.7173, 'grad_norm': 2.5819590091705322, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6725, 'grad_norm': 4.234008312225342, 'learning_rate': 4.75e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6337, 'grad_norm': 2.9719910621643066, 'learning_rate': 4.7e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6264, 'grad_norm': 3.659611701965332, 'learning_rate': 4.65e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6565, 'grad_norm': 4.071549415588379, 'learning_rate': 4.6e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6193, 'grad_norm': 3.2341091632843018, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6459, 'grad_norm': 3.1526639461517334, 'learning_rate': 4.5e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6112, 'grad_norm': 4.757175922393799, 'learning_rate': 4.45e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.66, 'grad_norm': 5.949135780334473, 'learning_rate': 4.4e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6349, 'grad_norm': 6.535569190979004, 'learning_rate': 4.35e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6128, 'grad_norm': 3.122844934463501, 'learning_rate': 4.2999999999999995e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.642, 'grad_norm': 3.838043212890625, 'learning_rate': 4.250000000000001e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6165, 'grad_norm': 3.3867406845092773, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6655, 'grad_norm': 7.206087112426758, 'learning_rate': 4.15e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6736, 'grad_norm': 7.343769073486328, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.22}\r\n",
      "{'loss': 0.6029, 'grad_norm': 6.669946193695068, 'learning_rate': 4.05e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.647, 'grad_norm': 3.465482234954834, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6215, 'grad_norm': 6.008103370666504, 'learning_rate': 3.95e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6494, 'grad_norm': 4.8838372230529785, 'learning_rate': 3.9e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6781, 'grad_norm': 2.88045334815979, 'learning_rate': 3.85e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6031, 'grad_norm': 4.240296840667725, 'learning_rate': 3.8e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.662, 'grad_norm': 4.275223731994629, 'learning_rate': 3.75e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6474, 'grad_norm': 5.416989326477051, 'learning_rate': 3.7e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6638, 'grad_norm': 4.887698173522949, 'learning_rate': 3.6499999999999998e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6989, 'grad_norm': 6.211803436279297, 'learning_rate': 3.6e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6259, 'grad_norm': 3.887589693069458, 'learning_rate': 3.55e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6771, 'grad_norm': 3.2601919174194336, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6786, 'grad_norm': 5.026573181152344, 'learning_rate': 3.4500000000000004e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6408, 'grad_norm': 5.2920403480529785, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6447, 'grad_norm': 6.758498668670654, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6657, 'grad_norm': 6.679663181304932, 'learning_rate': 3.3e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6297, 'grad_norm': 2.259446382522583, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6313, 'grad_norm': 4.584450721740723, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6187, 'grad_norm': 10.88632583618164, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.663, 'grad_norm': 15.293013572692871, 'learning_rate': 3.1e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6431, 'grad_norm': 7.553633213043213, 'learning_rate': 3.05e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6554, 'grad_norm': 3.515612840652466, 'learning_rate': 3e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6179, 'grad_norm': 6.130423069000244, 'learning_rate': 2.95e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.656, 'grad_norm': 4.492673873901367, 'learning_rate': 2.9e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6145, 'grad_norm': 4.739172458648682, 'learning_rate': 2.8500000000000002e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6829, 'grad_norm': 3.99406361579895, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6418, 'grad_norm': 6.833126544952393, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6611, 'grad_norm': 9.060272216796875, 'learning_rate': 2.7e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6563, 'grad_norm': 10.921353340148926, 'learning_rate': 2.65e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6146, 'grad_norm': 8.145242691040039, 'learning_rate': 2.6e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6537, 'grad_norm': 13.589674949645996, 'learning_rate': 2.55e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.666, 'grad_norm': 6.216100215911865, 'learning_rate': 2.5e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6739, 'grad_norm': 7.7049031257629395, 'learning_rate': 2.4500000000000003e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6825, 'grad_norm': 5.984519958496094, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6716, 'grad_norm': 10.51679515838623, 'learning_rate': 2.35e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6552, 'grad_norm': 15.406219482421875, 'learning_rate': 2.3e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.5852, 'grad_norm': 5.531635761260986, 'learning_rate': 2.25e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6318, 'grad_norm': 11.851587295532227, 'learning_rate': 2.2e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6607, 'grad_norm': 7.765085697174072, 'learning_rate': 2.1499999999999997e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6363, 'grad_norm': 8.232807159423828, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6562, 'grad_norm': 4.614868640899658, 'learning_rate': 2.0500000000000003e-06, 'epoch': 0.23}\r\n",
      "{'loss': 0.6305, 'grad_norm': 11.648334503173828, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6356, 'grad_norm': 7.192747116088867, 'learning_rate': 1.95e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6866, 'grad_norm': 3.8632705211639404, 'learning_rate': 1.9e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6525, 'grad_norm': 12.510650634765625, 'learning_rate': 1.85e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6185, 'grad_norm': 6.7042036056518555, 'learning_rate': 1.8e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6348, 'grad_norm': 6.146639347076416, 'learning_rate': 1.7500000000000002e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6821, 'grad_norm': 6.129841327667236, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6389, 'grad_norm': 16.01740074157715, 'learning_rate': 1.65e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.616, 'grad_norm': 8.838224411010742, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6445, 'grad_norm': 8.872451782226562, 'learning_rate': 1.55e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6212, 'grad_norm': 6.3334269523620605, 'learning_rate': 1.5e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6299, 'grad_norm': 7.795451641082764, 'learning_rate': 1.45e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.646, 'grad_norm': 8.153482437133789, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6209, 'grad_norm': 5.852598190307617, 'learning_rate': 1.35e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6286, 'grad_norm': 13.396305084228516, 'learning_rate': 1.3e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6398, 'grad_norm': 5.453277587890625, 'learning_rate': 1.25e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6256, 'grad_norm': 10.063995361328125, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6238, 'grad_norm': 10.854798316955566, 'learning_rate': 1.15e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6202, 'grad_norm': 5.76552677154541, 'learning_rate': 1.1e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6658, 'grad_norm': 9.692092895507812, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6776, 'grad_norm': 4.2036614418029785, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.24}\r\n",
      "{'loss': 0.6128, 'grad_norm': 9.616730690002441, 'learning_rate': 9.5e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6349, 'grad_norm': 10.633926391601562, 'learning_rate': 9e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6651, 'grad_norm': 6.688940048217773, 'learning_rate': 8.500000000000001e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.7002, 'grad_norm': 14.315378189086914, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6084, 'grad_norm': 9.633986473083496, 'learning_rate': 7.5e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6329, 'grad_norm': 8.886314392089844, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6429, 'grad_norm': 12.767714500427246, 'learning_rate': 6.5e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6432, 'grad_norm': 8.750144004821777, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6512, 'grad_norm': 8.158703804016113, 'learning_rate': 5.5e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6521, 'grad_norm': 11.673149108886719, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6672, 'grad_norm': 9.2394380569458, 'learning_rate': 4.5e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6671, 'grad_norm': 4.645927906036377, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6685, 'grad_norm': 11.022851943969727, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6427, 'grad_norm': 7.660468578338623, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6536, 'grad_norm': 4.391650676727295, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6843, 'grad_norm': 15.91461181640625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.666, 'grad_norm': 9.887001991271973, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6965, 'grad_norm': 8.99122142791748, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.24}\r\n",
      "{'loss': 0.6696, 'grad_norm': 7.240573406219482, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.24}\r\n",
      "{'loss': 0.5785, 'grad_norm': 10.524998664855957, 'learning_rate': 0.0, 'epoch': 0.24}\r\n",
      "{'train_runtime': 15724.785, 'train_samples_per_second': 16.28, 'train_steps_per_second': 0.064, 'train_loss': 0.32383872765302657, 'epoch': 0.24}\r\n",
      "100%|█████████████████████████████████████| 1000/1000 [4:22:04<00:00, 15.72s/it]\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =      0.2449\r\n",
      "  total_flos               = 124593750GF\r\n",
      "  train_loss               =      0.3238\r\n",
      "  train_runtime            =  4:22:04.78\r\n",
      "  train_samples            =     1045318\r\n",
      "  train_samples_per_second =       16.28\r\n",
      "  train_steps_per_second   =       0.064\r\n"
     ]
    }
   ],
   "source": [
    "# Continue from checkpoint\n",
    "!python run_clm.py --model_name_or_path \"gpt2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 1000 --per_device_train_batch_size 8 --gradient_accumulation_steps 32 --save_steps 500 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 0 --output_dir \"../results/gpt2_CDA_training_seed0_1kstep\" --persistent_dir \"/home/FYP/on0008an/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --resume_from_checkpoint \"../results/gpt2_CDA_training_seed0/checkpoint-500\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e9d4d6f09f369",
   "metadata": {},
   "source": [
    "## Phi 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425efefcbf93ff0",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb34702be3b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py --model_name_or_path \"microsoft/phi-2\" --tokenizer_name \"microsoft/phi-2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 42 --output_dir \"../results/CDA_FT/phi2/gender\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9753c78109354a",
   "metadata": {},
   "source": [
    "## Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42c73714707eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py --model_name_or_path \"microsoft/phi-2\" --tokenizer_name \"microsoft/phi-2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"religion\" --seed 42 --output_dir \"../results/CDA_FT/phi2/religion\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db72f210390430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 nohup python run_clm.py --model_name_or_path \"microsoft/phi-2\" --tokenizer_name \"microsoft/phi-2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"religion\" --seed 42 --output_dir \"../results/CDA_FT/phi2/religion\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e90345124e15c8",
   "metadata": {},
   "source": [
    "## Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228b38389242907",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 nohup python run_clm.py --model_name_or_path \"microsoft/phi-2\" --tokenizer_name \"microsoft/phi-2\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"race\" --seed 42 --output_dir \"../results/CDA_FT/phi2/race\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27624487fe693c",
   "metadata": {},
   "source": [
    "## For llama 2, max block size/sequence length is 1024 (OOM error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e8ceed1bac2",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15e372af447bd6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-21T18:58:40.120265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "06/22/2024 02:58:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py --model_name_or_path \"meta-llama/Llama-2-7b-hf\" --tokenizer_name \"meta-llama/Llama-2-7b-hf\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 42 --output_dir \"../results/CDA_FT/llama2/gender\" --persistent_dir \"/home/FYP/on0008an/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec3ecd39dd8179",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-26T22:34:38.540211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "06/27/2024 06:34:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "Padding token: </s>\r\n",
      "Quantization enabled\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.45s/it]\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\r\n",
      "\r\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\r\n",
      "  warnings.warn(message, FutureWarning)\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\r\n",
      "  warnings.warn(\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\r\n",
      "  warnings.warn(\r\n",
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\r\n",
      "  warnings.warn(\r\n",
      "06/27/2024 06:35:00 - WARNING - accelerate.utils.other - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\r\n",
      "[WARNING|trainer.py:591] 2024-06-27 06:35:00,725 >> max_steps is given, it will override any value given in num_train_epochs\r\n",
      "{'loss': 0.3438, 'grad_norm': 0.05251160264015198, 'learning_rate': 2.7475e-05, 'epoch': 0.02}\r\n",
      " 45%|██████████████████                      | 901/2000 [01:33<01:53,  9.65it/s]"
     ]
    }
   ],
   "source": [
    "# Continue from checkpoint\n",
    "!python run_clm.py --model_name_or_path \"meta-llama/Llama-2-7b-hf\" --tokenizer_name \"meta-llama/Llama-2-7b-hf\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 42 --output_dir \"../results/CDA_FT/llama2/gender\" --persistent_dir \"/home/FYP/on0008an/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora --resume_from_checkpoint \"../results/CDA_FT/llama2/gender/checkpoint-900\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff2fa9581b2035",
   "metadata": {},
   "source": [
    "## Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ae29c8a3113e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py --model_name_or_path \"meta-llama/Llama-2-7b-hf\" --tokenizer_name \"meta-llama/Llama-2-7b-hf\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"race\" --seed 42 --output_dir \"../results/CDA_FT/llama2/race\" --persistent_dir \"/home/ongzhiyang/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36736949ac0fef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-21T07:44:04.430023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/FYP/on0008an/.conda/envs/RunJupyter/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "06/21/2024 15:44:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\r\n"
     ]
    }
   ],
   "source": [
    "# Continue from checkpoint\n",
    "!python run_clm.py --model_name_or_path \"meta-llama/Llama-2-7b-hf\" --tokenizer_name \"meta-llama/Llama-2-7b-hf\" --do_train --train_file \"../data/wikipedia-10.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"gender\" --seed 42 --output_dir \"../results/CDA_FT/llama2/gender\" --persistent_dir \"/home/FYP/on0008an/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora --resume_from_checkpoint \"../results/CDA_FT/llama2/gender/checkpoint-200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df91138e3c55f3",
   "metadata": {},
   "source": [
    "## Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18a7fad2b2f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py --model_name_or_path \"meta-llama/Llama-2-7b-hf\" --tokenizer_name \"meta-llama/Llama-2-7b-hf\" --do_train --train_file \"../data/wikipedia-10_sample.txt\" --max_steps 2000 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --save_steps 100 --preprocessing_num_workers 1 --counterfactual_augmentation \"religion\" --seed 42 --output_dir \"../results/CDA_FT/llama2/religion\" --persistent_dir \"/home/FYP/on0008an/bias-bench-main\" --num_train_epochs 3 --overwrite_output_dir --logging_strategy \"steps\" --logging_steps 1 --block_size 1024 --lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fyp] *",
   "language": "python",
   "name": "conda-env-fyp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
